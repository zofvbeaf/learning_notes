## 数据分片与路由

+ **横向扩展**：提高单机硬件资源配合来解决问题。
+ **水平扩展：**增加机器的数量。
+ 对于待处理的海量数据，需要通过数据分片将数据进行切分，并分片到各个机器中；分片之后，需要找到某条记录
  的存储位置，这被称为数据路由。
+ 对于海量数据，通过数据分片实现系统的水平扩展，而通过数据复制来保证数据的高可用。
+ 通过将一份数据复制多次，提高系统的可用性；数据复制还可以增加读的效率，客户端可以从多个备份数据中选择物理距离较近的进行读取，即增加了读操作的并发性又可以提高单次读的效率。
+ 由于每份数据存在多个副本，在并发对数据进行更新时如何保证数据的一致性就成为了关键问题。
+ 常见的分片方式有：**哈希分片、范围分片**等

### 抽象模型

+ 实际上是一个**二级映射**关系：
  + 第一级映射是`key-partition`映射，其将数据记录映射到数据分片空间，这往往是多对一的映射关系，即一个数
    据分片包含多天记录数据；
  + 第二级映射是`partition-machine`映射，其将数据分片映射到物理机器中，这一版也是多对一映射关系，即一台
    物理主机容纳多个数据分片。
  + 在做数据分片时，根据`key-partition`映射关系将大数据水平切割成中多的数据分片，然后再按照`partition-machine`映射关系将数据分片放置到对应的物理机器上。
  + 在数据路由时，比如要查找某条记录的值`get(key)`，首先根据`key-partition`映射找到对应的数据分片，然后再查找`partition-machine`关系表，就可以知道具体哪台物理机器存储该条数据，之后即可从相应的物理机读取`key`对应的`value`内容

### 哈希取模法

+ `Round Robin`就是 哈希取模法。假设有K台物理机，通过以下哈希函数即可实现数据分片： `H(key)=hash(key)modK`对物理机进行编号`0`到`K-1`，根据以上哈希函数，对于以`key`为主键的某个记录，`H(key)`的数值即是物理机在在集群中的放置位置（编号）。
+ 抽象模型中第二级`partion-machine`映射为一对一映射。
+ 问题：**缺乏灵活性**，因为一旦集群中加入了某台机器或减少某台机器都会导致映射关系被打乱，需要重新分片。

### 虚拟桶

+ 为了解决哈希取模法缺乏灵活性的问题，在Round Robin基础上， 加入了一个“虚拟桶层”。所有记录都通过哈希函数映射到对应的虚拟桶中（多对一映射）。虚拟桶和物理机之间再由一层映射。

  ![1564623143898](https://ws1.sinaimg.cn/large/77451733gy1g5jxc3iwqhj20g20d73zj.jpg)

#### redis中分片的实现

+ **todo**

### 一致性哈希（Consistent Hashing）

+ 判断哈希算法好坏
  + **平衡性：**指的是哈希的结果应该尽可能负载均衡地分布到所有的节点中去，这样可以充分利用所有的节点空间。
  + **单调性：**指的是如果已经有一些内容通过哈希分配到了相应分片中，此时又有新的分片加入系统中，这可能导致哈希算法改变，哈希的结果应该保证原本已分配的数据被映射到原有的分片或新的分片中，而不会映射到旧分片集合中的其他分片。
  + **分散性：**在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致**相同内容被存储到不同缓冲中去，降低了系统存储的效率。**分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。
  + **负载：**负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么**对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容**。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。
+ 采用**哈希取模法**中，当有节点新增或者下线时，会**违反单调性**原则。
+ 环形`Hash`空间
  + 按照一定的哈希算法将对应的`key`值哈希到一个具有`2^32`个桶的空间中，即`0~（2^32-1）`。这个集合集合首尾相连，形成一个环。
  + 对象通过哈希函数计算出对应的`key`值，然后散列到环上。
  + 机器加入集群时，也需要通过哈希算法映射到环中（一般情况下，使用机器的IP地址作为哈希算法的输入）。环中的对象，存储于顺时针方向离自己最近的节点中。
+ 机器增加
  + 当往集群中添加一个新的节点，通过哈希算法得到哈希值`NODE-NEW`，并映射到环中。此时涉及到数据迁移。新节点的加入，会将原有的节点`NODE-OLD`的值域空间一分为二，其中顺时针方向离`NODE_NEW`更近的数据需要从`NODE-OLD`迁移至`NODE-NEW`。
  + 一致性算法在**保持了单调性**的同时，尽量减少了数据的迁移，减少了服务器的压力，适合于分布式集群。
+ 机器删除
  + 删除节点`NODE`时，其上数据顺时针迁移到下一个节点`NODE`中。
+ **平衡性**
  + 上述描述的一致性算法，可能会导致负载不均衡。在一致性哈希算法中，为了尽量满足负载均衡，引入了**虚拟节点**的概念。
  + 虚拟节点可以理解为一层抽象，每台机器/节点会包含若干虚拟节点，虚拟节点中存储数据，通过一定的虚拟节点
    分配规则可实现负载均衡。
+ 路由问题
  + 在`p2p`环境中没有中心管理节点，如何根据数据记录的主键以及哈希函数`H`定位到记录的内容？
    + a. 顺序查找
      + 接收到查询请求的节点根据哈希函数，获得待查找主键的哈希值`j`，首先判断是否在自身管理范围内，如果不在，则转交给后续节点继续查找，如此循环，直到找到某个机器节点`Nx`，`x`是大于等于`j`的最小编号的节点，这样最多需要遍历所有机器节点才会给出查找结果。
      + 顺序查找的时间开销太大：`O(n)`，为了解决方法`a`中时间开销过大的问题，采取路由表的做法。
    + b. 全局路由表
      + 全局路由表即每个节点上都会记录集群中所有节点的哈希值域范围。这样在任意节点上，最多只需遍历一次该全局路由表即可得到目标节点位置。
      + 但存在的问题：
        + 当有新增节点或者删除节点时，所有的节点需要更新该路由表
        + 集群中所有节点都要存储全局路由表，存储开销较大
    + c. 二分路由表法
      + 为了加速查找速度、避免太多存储空间，采用二分路由法。每个机器存储`m`条路由信息（`m`为哈希空间的二进制数值比特位长度）。第`i`条（从`1`开始）记录表示，存储距离自己 `2^(i - 1)` 的位置的节点编号。
      + 查询流程：
        + 接收到查询请求的节点根据哈希函数，获得待查找主键的哈希值j，首先判断是否在 `c<j<=s` （`Nc`为当前节点，`Ns`表示后续节点）。
        + 如果不在，查询该路由表，找到小于j的最大编号节点`Nh`。然后去`Nh`上查询。继续第一步进行查找。

### 范围分片

+ 范围分片中首先需要将所有记录的主键进行排序，然后在排序好的主键空间里将记录划分成数据分片，每个数据分片存储有序的主键空间片段内的所有记录。

## 数据复制与一致性

### CAP理论

+ `Consistency`（一致性）：指数据在多个副本之间能够保持一致的特性（严格的一致性）
+ `Availability`（可用性）：指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）
+ `Partition tolerance`（分区容错性）：分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供的服务，除非整个网络环境都发生了故障
+ 分区的概念
  + 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。
  + 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。

#### 论证

+ 如图所示，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，他们之间网络可以连通，N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzb4axhrj205o06oq37.jpg)

+ 在满足一致性的时候，N1和N2中的数据是一样的，V0=V0。
+ 在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。
+ 在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的
  正常运作。
+ 如图所示，这是分布式系统正常运转的流程，用户向N1机器请求数据更新，程序A更新数据库V0为V1。分布式系统将数据进行同步操作M，将V1同步的N2中V0，使得N2中的数据V0也更新为V1，N2中的数据再响应N2的请求。

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzhxascqj20h306rgn7.jpg)

+ 根据CAP原则定义，系统的一致性、可用性和分区容错性细分如下：
  + 一致性：N1和N2的数据库V之间的数据是否完全一样。
  + 可用性：N1和N2的对外部的请求能否做出正常的响应。
  + 分区容错性：N1和N2之间的网络是否互通。
+ 这是正常运作的场景，也是理想的场景。作为一个分布式系统，它和单机系统的最大区别，就在于网络。现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常。相当于要满足分区容错性，能不能同时满足一致性和可用性呢？还是说要对他们进行取舍？

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzjcx4z4j20h706v0ug.jpg)

+ 假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1。由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0。这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？
+ 这里有两种选择：
  + 第一：牺牲数据一致性，保证可用性。响应旧的数据V0给用户。
  + 第二：牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。
  + 这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。

#### 权衡

+ CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一（即**保证CP或AP**）。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。但是，对于**大多数互联网应用**来说，因为规模比较大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有**舍弃一致性来保证服务的 AP**。但是对于一些**金融相关行业**，它有很多场景需要确保一致性，这种情况通常会**权衡 CA 和 CP** 模型，**CA 模型网络故障时完全不可用**，**CP模型具备部分可用性。**
+ 在一个分布式系统中，对于这三个特性，我们只能三选二，无法同时满足这三个特性，三选二的组合以及这样系统
  的特点总结如下：
  + **CA** (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提
    交”（2PC）。CA 系统不能容忍网络错误或节点错误，**一旦出现这样的问题**，整个系统就会拒绝写请求，因为
    它并不知道对面的那个结点是否挂掉了，还是只是网络问题。**唯一安全的做法就是把自己变成只读的**。
  + **CP** (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，
    比如：**Paxos** 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有
    同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。
  + **AP** (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致
    性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。
+ 对于分布式系统分区容忍性是天然具备的要求，否则一旦出现网络分区，系统就拒绝所有写入只允许可读，这对大
  部分的场景是不可接收的，因此，在设计分布式系统时，更多的情况下是选举 CP 还是 AP，要么选择强一致性弱可
  用性，要么选择高可用性容忍弱一致性。
+ 传统的CAP具有误导性：
  + 在实际系统中，网络分区出现的概率很小，应该尽可能兼顾CAP，而不是在设计之初为了容忍小概率事件就放弃A或C。
  + 必须在AC之间做出取舍的时候，不应该粗粒度地在整个系统级别进行取舍。应该考虑系统中的不同子系统，
  + 即对不同子系统采取不同的取舍策略，不同的系统运行时对不同数据采取不同的取舍策略。
  + **CAP三者并非绝对的有或没有，应该看作连续变量。**
+ 推荐的CAP策略：
  + 在绝大多数系统未产生网络分区的情况下，应该尽可能保证AC两者兼得，也即大多数情况下应该保证CAP兼得；当网络分区发生之后，系统应该识别这种状况并对其进行正确的处理，具体而言，分为三个步骤：首先能够识别网络分区的发生，然后在网络分区场景下进入明确的分区模式，此时很可能会限制某些系统操作，最后在网络分区解决后能够进行善后工作，即恢复数据的一致性或者弥补分区模式中产生的错误。

### 一致性模型的分类

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzuvdiddj20k4092dih.jpg)

+ 强一致性

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5jzx5xyk4j20di063acf.jpg)

  + 对于连接到数据库的所有进程，看到的关于某数据的数据值是一致的，如果某进程对数据进行修改，所有进程的后续的读操作就会读到这个更新后的值为基准，直到整个数据被其他进程改变为止。
  + 但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。

+ 弱一致性

  + 不能满足强一致性的情形都可以称为弱一致性。
  + 即系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。

+ 最终一致性

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5jzyxyajij20dn06r0vz.jpg)

  + 最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一段时
    间，在这个时间之后可以保证这一点，而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为**不一致窗口**。
  + 不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。
  + 最终一致性在实际应用中又有多种变种
    + **因果一致性**：如果 A 进程在更新之后向 B 进程通知更新的完成，那么 B 的访问操作将会返回更新的值。而没有因果关系的 C 进程将会遵循最终一致性的规则（C 在不一致窗口内还是看到是旧值）。
    + **读你所写一致性**：因果一致性的特定形式。一个进程进行数据更新后，会给自己发送一条通知，该进程后续的操作都会以最新值作为基础，而其他的进程还是只能在不一致窗口之后才能看到最新值。
    + **会话一致性：**读你所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程可以读取到最新值，但如果会话终止，重新连接后，如果此时还在不一致窗口内，还是可能读取到旧值。
    + **单调读一致性：**如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。
    + **单调写一致性：**系统保证对同一个进程的写操作串行化。

### 两阶段提交（2PC）

+ 二阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保**证在分布式事务中**，要么所有参与进程都提交事务，要么都取消事务，即**实现 ACID 的原子性（A）**。**在数据一致性中**，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来**保证数据的强一致性**。

+ 2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： **参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。**

+ 顾名思义，`2PC` 分为两个过程：

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k0m0vg6kj21kx0nxgpv.jpg)

  + **表决阶段**：此时 `Coordinator` （协调者）向所有的参与者发送一个 `vote request`，参与者在收到这请求后，如果准备好了（锁定表，预写日志）就会向 `Coordinator` 发送一个 `VOTE_COMMIT` 消息作为回应，告知 `Coordinator` 自己已经做好了准备，否则会返回一个 `VOTE_ABORT` 消息；
  + **提交阶段**：`Coordinator` 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么`Coordinator` 就会发送 `GLOBAL_COMMIT` 消息，否则发送 `GLOBAL_ABORT` 消息；对于参与者而言，如果收到`GLOBAL_COMMIT` 消息，就会提交本地事务，否则就会取消本地事务。

+ 系统中存在三个阻塞状态：

  + 协调者的`WAIT`状态，参与者的`INIT`状态和`READY`状态。因为这三个状态都在等待对方的反馈消息。

+ 如何避免或者减少进程崩溃导致处于阻塞态的对象进入长时间的等待？

  + 超时判断机制，可以解决协调者的`WAIT`状态和参与者的`INIT`的长时间阻塞。
  + 参与者互询机制，可以解决大部分情况下参与者的`READY`状态长时间阻塞的可能。

+ 超时判断机制：

  + 如果协调者处于`WAIT`状态，加入超时判断机制，可以假定参与者发生崩溃或者存在网络通信故障限制，协调者可以多播`Global-abort`消息取消事务。
  + 如果参与者处于`INIT`状态，说明参与者在等待协调者发出的`Vote-request`消息，超时未收到消息，可以在本地中止事务，向协调者发送`Vote-abort`消息。
  + 如果参与者处于`READY`状态，此时不能直接使用超时判断机制，因为超时直接中止事务可能导致事务处于不一致：协调者实际上发送了`Global-commit`，其他参与者已经提交了事务。
  + 处于`READY`状态并发现超时的参与者必须要搞清楚协调者发出的是哪种消息，引入**参与者互询**规则可以减缓这一问题。陷入困境的参与者`P`可以询问另外的参与者`Q`，根据`Q`的状态来决定自己应该做什么。
    + 如果参与者`Q`处于`COMMIT`状态，说明协调者已经`Global-commit`，此时`P`可以安全地提交。
    + 如果参与者`Q`处于`ABORT`状态，说明协调者已经`Global-abort`，此时`P`可以安全地将自身状态转换`ABORT`。
    + 如果参与者`Q`处于`INIT`状态，如果考虑到协调者超时机制，最后该事务一定会进入`ABORT`状态，此时`P`可以安全地将自身状态转换成`ABORT`。
    + 如果参与者`Q`处于`READY`状态，此时无法从`Q`获得有效信息，询问其他节点。如果所有参与者都是处于`READY`状态，所有参与者必须处于阻塞状态，等待崩溃的协调者重新启动。即出现“**长时间阻塞**”。

#### 2PC一致性问题

+ 2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。
  + Coordinator 挂了，参与者没挂
    
    + 这种情况其实比较好解决，只要找一个 Coordinator 的替代者。当他成为新的 Coordinator的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。
  + 参与者挂了（无法恢复），Coordinator 没挂
    
    + 如果挂了之后没有恢复，那么是不会导致数据一致性问题。
  + 参与者挂了（后来恢复），Coordinator 没挂
    
    + 恢复后参与者如果发现有未执行完的事务操作，直接取消，然后再询问 Coordinator 目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。
  + 参与者挂了，Coordinator 也挂了
    + Coordinator 和参与者在第一阶段挂了
      + 由于这时还没有执行 commit 操作，新选出来的 Coordinator 可以询问各个参与者的情况，再决定是进行 commit 还是 roolback。因为还没有 commit，所以不会导致数据一致性问题。
    
    + Coordinator 和参与者在第二阶段挂了，但是挂的这个参与者在挂之前还没有做相关操作
      + 这种情况下，当新的 Coordinator 被选出来之后，他同样是询问所有参与者的情况。只要有机器执行了 abort（roolback）操作或者第一阶段返回的信息是 No 的话，那就直接执行 roolback 操作。如果没有人执行 abort 操作，但是有机器执行了 commit 操作，那么就直接执行 commit 操作。这样，当挂掉的参与者恢复之后，只要按照 Coordinator 的指示进行事务的 commit 还是 roolback 操作就可以了。因为挂掉的机器并没有做 commit 或者 roolback 操作，而没有挂掉的机器们和新的 Coordinator 又执行了同样的操作，那么这种情况不会导致数据不一致现象。
    + Coordinator 和参与者在第二阶段挂了，挂的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。
      + 这种情况下，新的 Coordinator 被选出来之后，如果他想负起 Coordinator 的责任的话他就只能按照之前那种情况来执行 commit 或者 roolback 操作。这样新的 Coordinator 和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了 commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他已经执行完了之前的事务，如果他执行的是 commit 那还好，和其他的机器保持一致了，万一他执行的是 roolback 操作呢？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和 Coordinator 通信，再想办法把数据搞成一致的，但是，**这段时间内他的数据状态已经是不一致的了**！
      + 所以，**2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。**为了解决这个问题，衍生出了3PC。

#### 2PC的优缺点

+ 优点：原理简洁清晰、实现方便；
+ 缺点：同步阻塞、单点问题、某些情况可能导致数据不一致。
+ 关于这几个缺点，在实际应用中，都是对 2PC 做了相应的改造：
  + **同步阻塞：**2PC 有几个过程（比如 Coordinator 等待所有参与者表决的过程中）都是同步阻塞的，在实际的应用中，这可能会导致长阻塞问题，这个问题是通过超时判断机制来解决的，但并不能完全解决同步阻塞问题；
  + **Coordinator 单点问题：**实际生产应用中，Coordinator 都会有相应的备选节点；
  + **数据不一致：**这个在前面已经讲述过了，如果在第二阶段，Coordinator 和参与者都出现挂掉的情况下，是有可能导致数据不一致的。

### 三阶段提交（3PC）

+ 三阶段提交协议（Three-Phase Commit， 3PC）最关键要解决的就是 阻塞问题和`Coordinator` 、参与者同时挂掉导致数据不一致的问题，所以 `3PC` 把在 `2PC` 中又添加一个阶段，这样三阶段提交就有：`CanCommit`、`PreCommit` 和`DoCommit` 三个阶段。
  

![](https://ws1.sinaimg.cn/large/77451733gy1g5k7vdfkmnj20gz08xmxk.jpg)

+ 阶段一 **CanCommit**

  + **事务询问：**Coordinator 向各参与者发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；
  + 参与者向 Coordinator **反馈询问的响应**：参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No。

+ 阶段二 **PreCommit**

  + **执行事务预提交：**如果 Coordinator 接收到各参与者反馈都是Yes，那么执行事务预提交：
    + **发送预提交请求：**Coordinator 向各参与者发送 preCommit 请求，并进入 prepared 阶段；
    + **事务预提交：**参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；
    + 各参与者向 Coordinator **反馈事务执行的响应**：如果各参与者都成功执行了事务操作，那么反馈给协调者 ACK响应，同时等待最终指令，提交 commit 或者终止 abort，结束流程；
  + **中断事务：**如果任何一个参与者向 Coordinator 反馈了 No 响应，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务。
    + 发送中断请求：Coordinator 向所有参与者发送 abort 请求；
    + 中断事务：无论是收到来自 Coordinator 的 abort 请求，还是等待超时，参与者都中断事务。

+ 阶段三 **doCommit**

  + 执行提交
    + 发送提交请求：假设 Coordinator 正常工作，接收到了所有参与者的 ack 响应，那么它将从预提交阶段进入提交状态，并向所有参与者发送 doCommit 请求；
    + 事务提交：参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；
    + 反馈事务提交结果：参与者完成事务提交后，向 Coordinator 发送 ACK 信息；
    + 完成事务：Coordinator 接收到所有参与者 ack 信息，完成事务。
  + 中断事务：假设 Coordinator 正常工作，并且有任一参与者反馈 No，或者在等待超时后无法接收所有参与者的反
    馈，都会中断事务
    + 发送中断请求：Coordinator 向所有参与者节点发送 abort 请求；
    + 事务回滚：参与者接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资
      源；
    + 反馈事务回滚结果：参与者在完成事务回滚之后，向 Coordinator 发送 ack 信息；
    + 中断事务：Coordinator 接收到所有参与者反馈的 ack 信息后，中断事务。

+ 3PC 虽然解决了 Coordinator 与参与者都异常情况下导致数据不一致的问题，3PC 依然带来其他问题：比如，网络分区问题，在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit。而且由于 3PC 的设计过于复杂，在解决 2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。

### 一致性协议 raft

+ `Raft`是一种用来管理复制日志的一致性算法: 通过选举一个`Leader`, 然后给予它管理复制日志的全部责任来实现一致性.
+ `Leader`从客户端接收日志条目, 把日志条目复制到其他服务器上, 并且当保证安全性的时候告诉其他的服务器把日志条目应用(apply)到它们的状态机中. `Leader`的设置简化了对复制日志的管理. 例如`Leader`可以决定新的日志条目放在日志中的位置(索引值)而不需要与其他服务器商议, 并且**数据都从Leader流向其他服务器.**
+ 可以容忍：`fail-stop (not Byzantine), delayed/lost messages`，不能容忍消息篡改
+ 通过 `Leader,` `Raft` 将一致性问题分解成了三个相对独立的子问题, 这些问题会在接下来的子章节中进行讨论:
  + `Leader` 选举：当前的 `Leader` 宕机时, 一个新的 `Leader` 需要被选举出来
  + 日志复制：`Leader`必须从客户端接收日志然后复制到集群中的其他节点, 并强制要求其他节点的日志保持和自己相同.
  + 安全性：在 `Raft` 中安全性的关键是保证状态机安全: 如果有任何的服务器节点已经把一个确定的日志条目应用到它的状态机中, 那么其他服务器节点不能再同一个日志索引位置应用一个不同的指令. 为了保证这个特性, 这个解决方案涉及到一个额外的选举机制上的限制。

#### raft宏观结构

![](https://ws1.sinaimg.cn/large/77451733gy1g5k84nwsbfj20zk0qgteg.jpg)

+ 如上图所示，为一个 Raft 集群，有三个 `RaftNode`节点，每个`RaftNode`即可以作为客户端，也可作为服务器。集群中`RaftNode`之间都是保持着连接。
+ 客户端可以连接其中任意一个节点，如果连接的不是`leader`，则`Follower`会拒绝请求，返回重定向，`Client`再连接到`Leader`。
+ 所有的客户端都连接到`Leader`，这样避免了转发，可以提升性能，但是随着系统的运行，Leader发生变化，需要调整连接。
+ 对于服务器转发模式，如果客户端可以接受一定的一致性折损，读请求可以不转发，直接在`RaftNode`上进行处理，这样返回的数据可能不是实时的，但是可以提高系统并行度，提升整体性能。

#### raftNode的细节

![](https://ws1.sinaimg.cn/large/77451733gy1g5k87yadirj20zk0e1tbc.jpg)

+ 首先`Local Server`接收到请求后，立即将请求日志附加到`SegmentedLog`中，这个日志会实时存到文件系统中，同时内存里也会保留一份。考虑到日志文件过大可能会影响性能和安全性，所以对日志做了分段处理，顺序分成多个文件，所以叫`SegmentedLog`。

#### raft基础

+ Raft 协议将 Server 进程分成三类，分别是 Leader ， Candidate ， Follower 。一个 Server 进程在某一时刻，只能是其中 一种类型，但这不是固定的。不同的时刻，它可能拥有不同的类型，一个 Server 进程的类型是如何改变的，后面会有解释。

+ 在一个由 Raft 协议组织的集群中有三类角色：

  + Leader（领袖）
  + Follower（群众）
  + Candidate（候选人）

+ 就像一个民主社会，领袖由民众投票选出。刚开始没有 领袖，所有集群中的 参与者 都是 群众，那么首先开启一轮大选。在大选期间 所有群众 都能参与竞选，这时所有群众的角色就变成了 候选人，民主投票选出领袖后就开始了这届领袖的任期，然后选举结束，所有除 领袖 的 候选人 又变回 群众角色 服从领袖领导。

+ 任期：`term`

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k8ozvyxij20m808zjsp.jpg)

+ 日志项 `LogEntry` 包括 `index`，`term`，`command` 三个元素。其中`index`为日志索引，`term`为任期，而`command`为具体的日志内容。
  
  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k8pilgd6j20zk0ag0tf.jpg)
  
+ 日志有四个重要的索引，分别是`firstLogIndex/lastLogIndex`和`commitIndex/applyIndex`，前两个就是当前内存中日志的开始和结束索引位置，后面两个是日志的提交索引和生效索引。之所以是用`firstLogIndex`而不是直接用零，是因为日志文件如果无限增长会非常庞大，`Raft`有策略会定时清理久远的日志，所以日志的起始位置并不是零。

+ `commitIndex`指的是过半节点都成功同步的日志的最大位置，这个位置之前的日志都是安全的可以应用到状态机中。Raft会将当前已经`commit`的日志立即应用到状态机中，这里使用`applyIndex`来标识当前已经成功应用到状态机的日志索引。

#### leader的选举过程

+ 一个最小的 `Raft` 民主集群需要三个参与者（如下图： A 、 B 、 C ），这样才可能投出多数票。

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k8t3ld1xj20jg0hfgoq.jpg)

+ 初始状态 ABC 都是 Follower ，然后发起选举，这时有 三种 可能的情形发生。下图中前二种都能选出 Leader ，第三种则表明 本轮投票无效（ Split Votes ）。对于第三种，每方都投给了自己，结果没有任何一方获得多数票。之后 每个参与方 随机休息一阵（ Election Timeout ）重新发起投票直到一方获得多数票。这里的关键就是**随机timeout** ，最先从 timeout 中恢复发起投票的一方，向还在 timeout 中的另外两方 请求投票，这时它就只能投给自己，导致很快达成一致。

+ `Raft`集群各个节点之间是通过`RPC`通讯传递消息的，每个节点都包含一个`RPC`服务端与客户端，初始时启动`RPC`服务端、状态设置为`Follower`、启动选举定时器，每个`Raft`节点的选举定时器超时时间都在`100-500`毫秒之间且并不一致；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k8uj1xw1j20cp05amxa.jpg)

+ `Raft`节点启动后在一个选举定时器周期内未收到心跳和投票请求，则状态转为候选者`candidate`状态、`term`自增、向`Raft`集群中所有节点发送投票请求并且重置选举定时器；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k8xap52qj20fk09xwf5.jpg)

+ `Raft`节点收到投票后对比当前`term`、`votedFor`、日志项信息判断觉得是否接受该投票请求，在此过程中如节点收到其他领导者的附加日志信息`RPC`请求如该`term`比自己大则接受该请求转为`Follower`状态，否则拒绝并保持候选人状态；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k98obxgkj20ek09uaax.jpg)

+ 当前由于每次选举超时发起投票请求都会增加term，而term又会导致Raft节点收到影响，所以出现网络分区后term增加到足够大后重新加入Raft集群时会导致集群可用性受到影响；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9agvhtfj20gl07b3z4.jpg)

+ 为了解决网络分区可能造成的影响这时在正式发起投票请求前引入了一个用于确认是否能成为`candidate`的`PrevoteRPc`请求，若能成功连接到集群中半数以上的节点，才能切换为`candidate`状态并发起新一轮的选举；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9biifr8j20g206i0t1.jpg)

+ 选出 Leader 后， Leader 通过 定期 向所有 Follower 发送 心跳信息 维持其统治。若 Follower 一段时间未收到 Leader 的 心跳，则认为 Leader 可能已经挂了，然后再次发起 选举 过程。

#### 日志复制

+ 日志复制可以说是`Raft`集群的核心之一，保证了`Raft`数据的一致性，下面通过几张图片介绍`Raft`集群中日志复制的逻辑与流程；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9efdnupj20f80ataam.jpg)

+ 在一个`Raft`集群中只有`Leader`节点能够接受客户端的请求，由`Leader`向其他`Follower`转发所有请求日志，并且有那么两条规则：`Leader`不删除任何日志、`Follower`只接收`Leader`所发送的日志信息；
  

![](https://ws1.sinaimg.cn/large/77451733gy1g5k9jf4sx0j20is0cfq3g.jpg)

+ 此图介绍了`Raft`集群中日志的组成结构，日志由序号与条目组成，每个条目又由任期与指令组成，`committed`范
  围内为已提交的日志是指过半节点已经接收并存储的日志；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9mrh2f2j20gb0bt0te.jpg)

+ 上图从整个上介绍了 `Raft` 集群的日志复制流程，`Leader` 接收到指令后写入到本地日志，在随后的心跳中（`AppendEntries`）往其他追随者发送该条目，等待收到过半追随者响应后将该条目标志位已提交状态，并发往状态机执行，完成后返回结果给客户端；在后续心跳包（`AppendEntries`）中通知所有追随者哪些条目为已提交状态，以便追随者更新在自己状态机中执行该指令； 只有`Leader`能够接受客户端的指令，追随者只能够接收领导者的`AppendEntries`请求；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9oj06msj20i207x74k.jpg)

+ 在`Raft`集群中可通过条目索引号、任期号唯一确定一个条目，该条目前序所有条目也是一致的，如上图中索引
  号为`5`的条目为已提交状态的条目，则从索引号`1`到`5`的所有条目均为已提交的状态；

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9q69notj20i608raaj.jpg)

+ 上图中`Leader`发送`AppendEntries`请求时带有其前序索引位置4、前序任期号2，发往`Follower1`、`Follower2`；`Follower1`由于前序索.引与前序任期能匹配本地条目所以将会接受该请求; `Follower2`由于前序索引与前序任期未能够匹配所以拒绝该请求;

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9ru220aj20lj0ait99.jpg)

+ `Raft`处理日志不一致的情况是通过强制追随者复制领导者日志来调整日志一致性的，所以当追随者与领导者出现日志不一致时，追随者日志将会被领导者日志覆盖；

+ 要使领导者与追随者保持一致性的状态，需要两者找到一致性的位置，删除追随者该位置之后所有日志条目，发送领导者日志给追随者； 领导者通过在每一个追随者维护了一个 `nextIndex`，表示下一个需要发送给跟随者的日志条目索引地址，领导者刚获得选举时，初始化所有 `nextIndex` 值为自己的最后一条日志的`index`加`1`；当追随者的日志和领导者不一致，那在下一次的`AppendEntries`时的一致性检查会失败，被追随者拒绝后，领导者就会减小`nextIndex` 值进行重试，`nextIndex` 会在某位置使领导者和追随者日志达成一致。 当日志达成一致时，追随者会接受该`AppendEntries`请求，这时追随者冲突的日志条目将全部被领导者的日志所覆盖。一旦`AppendEntries`成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。

#### 安全性

+ 前面描述了Raft是怎么选举和复制日志的，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。因此，Raft增加了一些限制来保证安全性。有一些前面可能已经提过。

+ **选举限制：**投票阻止没有全部日志条目的服务器赢得选举

  + 请求投票 `RPC` 实现了这样的限制： `RPC` 中包含了候选人的日志信息，然后`Follower`会拒绝掉那些日志没有自己新的投票请求。Raft通过比较两份日志中最后一条日志条目的索引值和任期号来定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。
  + 成为`Leader`需要获得大多数的投票，意味着需要比大多数节点的日志要新，而已提交的日志条目需要复制到大多数的节点，`Leader`能获得大多数的投票就意味着一定包含已提交的日志。

+ 永远不提交任期之前的日志条目（**只提交任期内的日志条目**）

  + `Raft`永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导人当前任期里的日志条目才会通过计算副本数目去提交。下面举例说明为什么：

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5k9y36fawj20fa078gq1.jpg)

  + 如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交：

  + 在 (a) 中，S1 是领导者，部分的复制了索引位置 2 的日志条目。

  + 在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。

  + 在(c)中，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。

  + 在(d)中，S1又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为S5 就不可能选举成功）。而且这样也同时保证了，之前的所有老的日志条目也会被提交。

  + 所以，若之前任期的日志已复制到大多数节点，即使未提交，Raft 也能保证最终会提交，并且，为了保证及时提交，新Leader上任后会立即往日志中追加一个空节点。

+ **Follower和Candidate崩溃**

  + 之前我们只讨论了Leader崩溃的情况，当Follower和Candidate崩溃时，Raft会不断的重试，如果崩溃的机器重启
    了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。**Raft的 RPCs 都是幂等的，所以这样重试不会造成任何问题**。

+ **与客户端的交互问题**

  + 与客户端的交互过程中，Leader可能会崩溃，或网络超时，客户端未收到响应而发起重试，这可能会引起指令重复执行等问题。所以，客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。
  + 即实现`linearizable`语义：保证对于一个对象的写操作一旦完成，需要立即被后续的读操作看到，即读操作一定是读到该对象的最新的值。
  + 对于客户端的**只读请求**：
    + `leader`节点响应客户端的请求时可能已经故障（或是已经发生了网络分区），集群中已经选出了新的`leader`节点，但是旧的`leader`节点自身还不知道。
    + 为了不返回脏数据，同时保证`linearizable`语义，`raft`协议在处理只读请求时，除了直接读取`leader`节点对应的状态信息，还需要使用额外的措施：
      + `leader`节点必须有关于已提交日志的最新信息。它会在任期开始时提交一条空日志记录。这样上一个任期中的所有日志都会被提交。
      + `leader`节点会记录该只读请求对应的编号作为`readIndex`，当leader节点的提交位置（`commitIndex`）达到或是超过该位置之后，即可响应该只读请求。
      + `leader`节点在处理只读请求之前必须检查集群中是否有新的`leader`节点，通过和半数以上的节点交换一次心跳来实现。
      + 随着日志的不断提交，leader节点的提交位置（`commitIndex`）最终会超过`readIndex`，此时`leader`就可以响应客户端的只读请求了。

#### Leader对一致性的影响

+ Raft 协议强依赖 Leader 节点的可用性，以确保集群数据的一致性。数据的流向只能从 Leader 节点向
  Follower 节点转移。具体过程如下：

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5kaftfppmj20jg0c9tam.jpg)

  + 当 Client 向集群 Leader 节点 提交数据 后， Leader 节点 接收到的数据 处于 未提交状态（ Uncommitted ）。
  + 接着 Leader 节点会 并发地 向所有 Follower 节点 复制数据 并 等待接收响应。
  + 集群中至少 超过半数 的节点 已接收 到数据后， Leader 再向 Client 确认数据 已接收。
  + 一旦向 Client 发出数据接收 Ack 响应后，表明此时 数据状态 进入 已提交（ Committed ）， Leader 节点再向 Follower 节点发通知告知该数据状态已提交。

+ 在这个过程中，主节点 可能在 任意阶段 挂掉，看下 Raft 协议如何针对不同阶段保障 数据一致性 的。

  + **情形1：**数据到达 Leader 节点前，这个阶段 Leader 挂掉不影响一致性，不用多说。

  + **情形2：**数据到达 Leader 节点，但未复制到 Follower 节点。

    + 这个阶段 Leader 挂掉，数据属于 未提交状态， Client 不会收到 Ack 会认为 超时失败 可安全发起 重试。
    + Follower 节点上没有该数据，重新选主 后 Client 重试 重新提交 可成功。原来的 Leader 节点 恢复 后作为Follower 加入集群，重新从 当前任期 的新 Leader 处 同步数据，强制保持和 Leader 数据一致。

  + **情形3：**数据到达 Leader 节点，成功复制到 Follower 所有节点，但 Follower 还未向 Leader 响应接收。

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5kb3qonnjj20jg0b0dhh.jpg)

    + 这个阶段 Leader 挂掉，虽然数据在 Follower 节点处于 未提交状态（ Uncommitted ），但是 保持一致 的。重新选出 Leader 后可完成 数据提交。
    + 此时 Client 由于不知到底提交成功没有，可重试提交。针对这种情况 Raft 要求 RPC 请求实现 幂等性，也就是要实现 内部去重机制。

  + **情形4：**数据到达 Leader 节点，成功复制到 Follower 的部分节点，但这部分 Follower 节点还未向 Leader 响应接收。

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5kb41hnnhj20jg0b8q4b.jpg)

    + 这个阶段 Leader 挂掉，数据在 Follower 节点处于 未提交状态（ Uncommitted ）且 不一致。
    + Raft 协议要求投票只能投给拥有 最新数据 的节点。所以拥有最新数据的节点会被选为 Leader ，然后再 强制同步数据 到其他 Follower ，保证 数据不会丢失并 最终一致。

  + **情形5：**数据到达 Leader 节点，成功复制到 Follower 所有或多数节点，数据在 Leader 处于已提交状态，但在 Follower 处于未提交状态。

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5kb9nlujwj20jg09rjsz.jpg)

    + 这个阶段 Leader 挂掉，重新选出 新的 Leader 后的处理流程和阶段 3 一样。

  + **情形6：**数据到达 Leader 节点，成功复制到 Follower 所有或多数节点，数据在所有节点都处于已提交状态，但还未响应Client。

    + 这个阶段 Leader 挂掉，集群内部数据其实已经是 一致的， Client 重复重试基于幂等策略对 一致性无影响。

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5kbar5zfhj20jg09v0ug.jpg)

  + **情形7：**网络分区导致的**脑裂**情况，出现双 Leader 的现象。

    + 网络分区 将原先的 Leader 节点和 Follower 节点分隔开， Follower 收不到 Leader 的 心跳 将 重新 发起选举产生新的 Leader ，这时就产生了 双Leader 现象。
    + 原先的 Leader 独自在一个区，向它提交数据不可能复制到多数节点所以永远提交不成功。向新的 Leader 提交数据可以提交成功。网络恢复 后，旧的 Leader 发现集群中有 更新任期（ Term ）的新 Leader ，则 自动降级 为 Follower 并从新Leader 处 同步数据 达成集群 数据一致。

#### 日志压缩

+ 在一个实际的分布式存储系统中，不可能让节点中的日志无限增加。冗长的日志导致系统重启时需要花费很长的时
  间进行回放，影响系统整体可用性。Raft 与 Chubby、Zookeeper 等类似，都采用了snapshot 技术进行日志压缩，丢弃 snapshot 之前的日志项目。

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5kbfra3afj20h80b9js4.jpg)

+ Raft 中每个节点独立的对自己的系统状态进行snapshot操作，当然只能对已经committed日志项(已经apply到了状态机)进行snapshot。snapshot有一些元数据，包括last_included_index，即snapshot覆盖的最后一条committed日志项的index，以及last_included_term，即这条日志的termid。这两个值在snapshot之后的第一条log entry的AppendEntriesRPC的consistency check的时候会被用上。一旦这个节点做完了snapshot，就可以把这条日志及之前的日志项目删除，压缩日志长度。

+ snapshot的缺点就是不是增量的，即使内存中某个值没有变，下次做snapshot的时候同样会被dump到磁盘。当leader需要发给某个follower的log entry被丢弃了(因为leader做了snapshot)，leader会将snapshot发给落后太多的follower。或者当新加进一台机器时，也会发送snapshot给它。

+ 做snapshot有一些需要注意的性能点

  + 不要做太频繁，否则消耗磁盘带宽。 
  + 不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响availability。系统推荐当日志达到某个固定的大小做一次snapshot。
  + 做一次snapshot可能耗时过长，会影响正常log entry的replicate。这个可以通过使用copy-on-write的技术来避免snapshot过程影响正常log entry的replicate。

#### 快照

+ 关于指定数据集合的一个完全可用拷贝，该拷贝包括相应数据在某个时间点（拷贝开始的时间点）的映像。快照可以是其所表示的数据的一个副本，也可以是数据的一个复制品。

+ 快照是完全可用的拷贝，但不是一份完整的拷贝，至于为什么，后面会详细讲。

+ 一般情况下，`etcd`的`raft`集群的每个节点都会在日志记录个数达到一个固定阈值的时候，触发一次创建快照的操作。

+ 存储快照的使用场景

  + 场景1：存储快照，是一种数据保护措施，可以对源数据进行一定程度的保护，通俗地讲，可以理解为----后悔药。
  + 场景2：快照是一份完全可用的副本，那么，它完全可以被上层业务当做源数据。
    + 针对源数据，创建快照后，将快照卷映射给其他上层业务，可以用于数据挖掘和开发测试等工作，针对快照的读操作不影响源卷的数据。
    + 这种功能，常用于直播（视频&图片）鉴黄、科研数据模拟开发测试等，比如，视频直播平台需要将某一段时间的视频提供给执法机构进行筛查分析，那么可以通过对特定时间点保存的数据创建快照，将快照映射给执法机构的业务主机去进行挖掘分析。

+ 存储快照的实现原理

  + 目前，快照的实现方式均由各个厂商自行决定，但主要技术分为2类，一种是写时拷贝COW（Copy On Write），另一种，是写重定向ROW（Redirect On Write）。

  + **写时拷贝COW**

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5kbv15ca8j210i0lxwff.jpg)

    + COW(Copy-On-Write)，写时拷贝，也称为写前拷贝。

    + 创建快照以后，如果源卷的数据发生了变化，那么快照系统会首先将原始数据拷贝到快照卷上对应的数据块中，然后再对源卷进行改写。

    + 写操作

      + 如上图简要示例，快照创建以后，若上层业务对源卷写数据X，X在缓存中排队，快照系统将X即将写入的位置（逻辑地址）上的数据Y，拷贝到快照卷中对应的位置（逻辑地址）上，同时，生成一张映射表，表中一列记录源卷上数据变化的逻辑地址，另一列记录快照卷上数据变化的逻辑地址。我们可以看到，上层业务每下发一个数据块，存储上，发生了两次写操作：一次是源卷将数据写入快照卷（即图中Y），一次是上层业务将数据写入源卷（即图中X）。

    + 读操作

      ![](https://ws1.sinaimg.cn/large/77451733gy1g5kdfnjsjlj21cg0lxmzc.jpg)

      + 如上图，快照卷若映射给上层业务进行数据分析等用途时，针对快照进行读操作时，首先由快照系统判断，上层业务需要读取的数据是否在快照卷中，若在，直接从快照卷读取，若不在，则查询映射表，去对应源卷的逻辑地中读取（这个查表并去源卷读的操作，也叫读重定向）。这一点，**恰好就解释了为什么快照是一份完全可用的副本，它没有对源卷进行100%的拷贝，**但对上层业务来说，却可以将快照看做是和源卷“一模一样”的副本。
      + 针对源卷进行读操作时，与快照卷没有数据交互。
      + 我们可以看到，快照对源卷的数据具有很好的保护措施，快照可以单独作为一份可以读取的副本，但并没有像简单的镜像那样，一开始就占用了和源卷一样的空间，而是根据创建快照后上层业务产生的数据，来实时占用必需的存储空间。

    + 快照回滚（rollback）：

      ![](https://ws1.sinaimg.cn/large/77451733gy1g5kdr9fcs9j21at0lxacy.jpg)

      + 回滚操作的前提条件是，锁定源卷（暂停对待回滚的逻辑地址上的IO操作），然后通过查映射表，将快照卷上的对应数据回拷到源卷中。

    + 快照删除：

      + 采用COW技术的快照，其源卷即保存着完整的实时数据，因此，删除快照时，直接销毁了快照卷和映射表，与源卷不存在数据交互。

  + **写时重定向ROW**

    + `ROW(Redirect-on-write)`，也称为写时重定向。

    + 创建快照以后，快照系统把对数据卷的写请求重定向给了快照预留的存储空间，直接将新的数据写入快照卷。上层业务读源卷时，创建快照前的数据从源卷读，创建快照后产生的数据，从快照卷读。

      ![](https://ws1.sinaimg.cn/large/77451733gy1g5kdwg44y7j21e70lxwhw.jpg)

    + 写操作

      + 如上图简要示例，快照创建以后，若上层业务对源卷写数据X，X在缓存中排队，快照系统判断X即将写入源卷的逻辑地址，然后将数据X写入快照卷中预留的对应逻辑地址中，同时，将源卷和快照卷的逻辑地址写入映射表，即写重定向。我们可以看到，上层针对源卷写入一个数据块X，存储上只发生一次写操作，只是写之前进行了重定向。

    + 读操作

      + 若快照创建以后，上层业务对**源卷**进行读，则有两种情况：
        + 若读取的数据，在创建快照前产生，数据是保存在源卷上的，那么，上层就从源卷进行读取；
        + 若需要读取的数据是创建快照以后才产生的，那么上层就查询映射表，从快照卷进行读取（即读重定向）。
      + 若快照创建以后，上层业务对**快照卷**进行读，同样也有两种情况：
        + 若读取的数据，在创建快照前产生，数据是保存在源卷上的，那么上层就查询映射表，从源卷进行读取；
        + 若需要读取的数据是创建快照以后才产生的，那么上层就直接从快照卷进行读取。我们可以看到，`ROW`快照也是根据创建快照后上层业务产生的数据，来实时占用必需的存储空间。

    + 快照回滚（`rollback`）：

      + 采用`ROW`技术的快照，其源卷始终保存着快照创建前的完整数据，快照创建后，上层业务产生的数据都写入了快照中，因此，快照的回滚只是取消了对源卷的读重定向操作。通俗地说，就是源卷上没有进行任何数据操作，上层业务对源卷的读，仅限于读源卷（即不会去读取快照卷的数据）。

    + 快照删除：

      ![](https://ws1.sinaimg.cn/large/77451733gy1g5ke330hyoj21e70lxgoe.jpg)

      + 采用ROW技术的快照，其源卷始终保存着快照创建前的完整数据，快照创建后，上层业务产生的数据都写入了快照中。因此，若要删除快照，必然要先**将快照卷中的数据，回拷到源卷中，拷贝完成才能删除**，如上图。此时我们可以设想，如果，针对一份源数据，在18:00创建了快照，上层业务持续产生大量新的数据，19:00又创建了快照，20:00又创建了快照……那么，在有多份快照的情况下，如果需要删除快照，就会出现，多个快照向源卷回拷数据的情况，可能导致回拷量非常大，耗时很长。

  + **COW**和**ROW**对比

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5ke5k8lwhj20zk0js7hk.jpg)

    + 如上表，COW的写时拷贝，导致每次写入都有拷贝操作，大量写入时，源卷的写性能会有所下降，而读源卷是不会受到任何影响的，删除快照时，只是解除了快照和源卷的关系，同时删除了快照卷的数据而已。ROW在每次写入仅做了重定向操作，这个操作耗时是几乎可以忽略不计的，源卷的写性能几乎不会受到影响，但读源卷时，则需要判断数据是创建快照前还是创建快照后，导致大量读时，性能受到一定影响，比较致命的是，若源卷有多个快照，在删除快照时，所有快照的数据均需要回拷到源卷才可以保证源卷数据的完整性。

## 分布式锁

+ 单进程同一进程中使用互斥锁访问临界区，保证同一时间内只有一个线程访问临界区；分布式的集群环境中，不同节点/不同进程访问共享资源，则使用分布式锁保证并发正确性。

+ 分布式锁的实现方案：

  + 基于数据库实现
  + 基于缓存（redis、memcached等）实现
  + 基于Zookeeper实现
  + 基于Chubby实现

+ 分布式锁的需求分析：

  + 可以保证在分布式部署的应用集群中，同一方法在同一时间只能被一台机器上的一个线程执行。
  + 锁可能需要重入（重入指的是 是否能被同一线程多次加锁）
  + 锁可能是阻塞的（此处阻塞指的是 加锁操作返回时，必须已经获得锁成功）
  + 获取、释放锁的性能要好

+ 基于数据库实现分布式锁

  + 最简单的方式就是创建一张锁表，然后通过操作该表中的数据来实现。当需要锁住某个方法或资源时，就在该表中增加一条记录，想要释放时就删除这条记录。

+ 基于`Redis`实现分布式锁

  + 先介绍Redis中三条指令：

    + `setnx(key, value)` ：命令在指定的key不存在时，为key设置指定的值，设置成功返回1，失败返回0。可用于加锁。
    + `del(key)` ：删除已经存在的键，不存在的key会被忽略。删除已存在的返回1，否则返回0。用于释放锁。
    + `expire(key, expired_time)` ：设置key的过期时间，key过期之后不再可用。设置成功返回1，当 key 不存在或者不能为 key 设置过期时间时返回 0 。

  + 第一版实现

    ```c++
    if(setnx(key，1) == 1){
        expire(key，30)
            try {
                do something ......
            } finally {
                del(key)
            }
    }
    ```

    + 问题：
      + **setnx与expire的组合非原子的：**如果setnx成功，然后突然宕机，此时没有加过期时间，会导致锁永远无法释放。
        + 解决措施：setnx指令本身是不支持传入超时时间的，但Redis 2.6.12以上版本为set指令增加了可选参数，伪代码如下： set（key，1，30，NX）
      + **达到超时时间导致的误删：**线程A执行得很慢，在过期时间内，都没有执行完，此时锁过期自动释放；线程B获得锁；线程A执行完毕，删除锁，但此时释放的是线程B加的锁。
        + 解决措施：为了避免超时未执行完毕自动删除，让获得锁的线程开启一个守护线程，用来给快要过期的锁“续航”。比如说，当过去了29秒，线程A还没执行完，这时候守护线程会执行expire指令，为这把锁“续命”20秒。守护线程从第29秒开始执行，每20秒执行一次。
        + 为了避免删除非自己线程id的锁，在加锁的时候把当前的线程id作为value，并在删除之前验证key对应的value是不是自己的id。

## 系统设计题

### [分布式ID生成器](https://chuansongme.com/n/2459549)

> 如何设计一个分布式ID生成器(Distributed ID Generator)，并保证ID按时间粗略有序？

+ 使用数据库的 auto_increment （定义列为自增的属性） 来生成全局唯一递增ID

  + 优点：

    + 简单，使用数据库已有的功能
    + 能够保证唯一性
    + 能够保证递增性
    + 步长固定

  + 缺点：

    + 可用性难以保证：数据库常见架构是一主多从+读写分离，生成自增ID是写请求，主库挂了就玩不转了
    + 扩展性差，性能有上限：因为写入是单点，数据库主库的写性能决定ID的生成性能上限，并且难以扩展

  + 改进方法：

    + 增加主库，避免写入单点，数据水平切分，保证各主库生成的ID不重复，但丧失了ID生成的"绝对递增性"

      ![](https://ws1.sinaimg.cn/large/77451733gy1g5l2br7qb5j208504laad.jpg)

    + 单点批量ID生成服务，使用批量的方式降低数据库写压力。保证了ID生成的绝对递增有序，但服务仍然是单点，生成ID可能会不连续，中间出现空洞。

+ 取当前毫秒数，如果并发量超过1000，会生成重复的ID

+ snowflake算法：一个long型的ID，使用其中41bit作为毫秒数，10bit作为机器编号，12bit作为毫秒内序列号。

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5l2o42pq3j20sd09m406.jpg)

+ 如何保证ID严格有序呢？

  + 在分布式这个场景下，是做不到的，要想高性能，只能做到粗略有序，无法保证严格有序。

### 短网址服务（TinyURL）

+ 微博的短网址服务用的是长度为7的字符串，这个字符串可以看做是62进制的数，那么最大能表示62^7个网址，远远大于45亿(当前互联网上的网页总数)。所以长度为7就足够了。
  + 现代的web服务器（例如Apache, Nginx）大部分都区分URL里的大小写了，所以用大小写字母来区分不同的URL是没问题的。
+ 一个长网址，对应一个短网址，还是可以对应多个短网址？
  + 应当一对多，一般而言，一个长网址，在不同的地点，不同的用户等情况下，生成的短网址应该不一样，这样，在后端数据库中，可以更好的进行数据分析。
  + 以这个7位长度的短网址作为唯一ID，这个ID下可以挂各种信息，比如生成该网址的用户名，所在网站，HTTP头部的 UserAgent等信息。
+ 如何计算短网址？
  + 最容易想到的办法是哈希，先hash得到一个64位整数，将它转化为62进制整，截取低7位即可。但是哈希算法会有冲突，如何处理冲突呢，又是一个麻烦。这个方法只是转移了矛盾，没有解决矛盾，抛弃。
  + 正确答案：使用分布式ID生成器
+ 如何存储？
  + 可以用任意一个分布式KV数据库，例如Redis, LevelDB。
+ 301还是302重定向
  + 这也是一个有意思的问题。这个问题主要是考察你对301和302的理解，以及浏览器缓存机制的理解。
  + 301是永久重定向，302是临时重定向。短地址一经生成就不会变化，所以用301是符合http语义的。但是如果用了301，Google，百度等搜索引擎，搜索的时候会直接展示真实地址，那我们就无法统计到短地址被点击的次数了，也无法收集用户的Cookie, User Agent 等信息，这些信息可以用来做很多有意思的大数据分析，也是短网址服务商的主要盈利来源。
  + 所以，正确答案是302重定向。
+ 预防攻击
  + 如果一些别有用心的黑客，短时间内向TinyURL服务器发送大量的请求，会迅速耗光ID，怎么办呢？
  + 首先，限制IP的单日请求总数，超过阈值则直接拒绝服务。
  + 光限制IP的请求数还不够，因为黑客一般手里有上百万台肉鸡的，IP地址大大的有，所以光限制IP作用不大。
  + 可以用一台Redis作为缓存服务器，存储的不是 ID->长网址，而是 长网址->ID，仅存储一天以内的数据，用LRU机制进行淘汰。这样，如果黑客大量发同一个长网址过来，直接从缓存服务器里返回短网址即可，他就无法耗光我们的ID了。

### 定时任务调度器

+ 实现一个定时任务调度器，有很多任务，每个任务都有一个时间戳，任务会在该时间点开始执行。例如Uber打车48小时后自动好评，淘宝购物15天后默认好评，等等。
+ **方案1: PriorityBlockingQueue + Polling**
  + 对于生产者，可以用一个 while(true) ，造一些随机任务塞进去
  + 对于消费者，起一个线程，在 while(true) 里每隔几秒检查一下队列，如果有任务，则取出来执行。
  + 总结起来就是轮询(polling)。轮询通常有个很大的缺点，就是时间间隔不好设置，间隔太长，任务无法及时处理，间隔太短，会很耗CPU。
+ **方案2: PriorityBlockingQueue + 时间差**
  + 可以把方案1改进一下， while(true) 里的逻辑变成：
    + 偷看一下堆顶的元素，但并不取出来，如果该任务过期了，则取出来
    + 如果没过期，则计算一下时间差，然后 sleep()该时间差
  + 不再是 sleep() 一个固定间隔了，消除了轮询的缺点。
  + 这个方案其实有个致命的缺陷，假设当前堆顶的任务在100秒后执行，消费者线程peek()偷看到了后，开始sleep 100秒，这时候一个新的任务插了进来，该任务在10秒后应该执行，但是由于消费者线程要睡眠100秒，这个新任务无法及时处理。
+ **方案3: DelayQueue**
  
  

## 分布式存储

### 分类

+ 分布式文件系统：
  + 存`Blob(binary large object)`对象、定长块、大文件。
  + 一般是将数据拆分成chunk来组织，处理数据的复制、一致性、负载均衡、容错等问题。
  + 如`facebook haystack`、`Taobao Filesystem(TFS)`，`GFS`，`Amazon EBS`。
+ 分布式键值系统
  + 存储半结构化数据，只提供`CRUD(Create/Read/Update/Delete)`功能。一般用作缓存。
  + 如`Amazon Dynamo`，`Taobao tair`，`Memcache`。
  + 数据分布常采用一致性哈希，典型如`Amazon DynamoDB`。
+ 分布式表格系统
  + 存储关系复杂的半结构化数据
  + 主要针对单张表格的操作，不支持复杂的多表操作（如`Join`）
  + 如`Bigtable`（存储结构化数据） ，`Megastore`，`Microsoft Azure Table Storage`，`Amazon DynamoDB`
+ 分布式数据库
  + 存储结构化数据
  + 多表复杂操作，支持事务以及并发控制

### 单机存储系统

#### 硬件基础

+ `CPU`架构
  
  + `SMP`：对称对处理机，所有资源共享，总线竞争大
  + `NUMA`：非一直存储访问，具有独立的本地内存，`I/O`槽口。开发程序时需要尽量减少不同`NUMA`节点之间的信息交互。
  
+ 常用硬件性能参数

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5tomm1zcrj20wd0eb78f.jpg)

+ 存储系统的性能主要包括两个维度：吞吐量以及访问时延，设计系统时要求能够在保证访问时延的基础上，通过最低的成本实现尽可能高的吞吐量。
+ `SSD`的随机读延迟较小，磁盘应尽量顺序写。`SSD`适合用来做缓存或用在存储小数据量且对性能要求较高的场合。
+ `SSD`存在**写放大**问题：因为不像磁盘可以直接写，`SSD`写入需要先擦除原来的数据，以`block`为单位，擦除的方式是先将此block移到别处，再将没有标记删除的数据拷贝回来。

#### 单机存储引擎

+ 哈希存储引擎：`Bitcask`

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5toxbogvfj20n50h1te8.jpg)
  + 定期合并
  + 快速恢复，内存中的索引转存到磁盘，方便重启后恢复

+ `B`树存储引擎

  + `Mysql Innodb`的特殊索引：聚集索引，行的数据存于其中，组织成`B+`树

    ![](https://ws1.sinaimg.cn/large/77451733gy1g5tprydbd1j20ob0c1jtw.jpg)

  + 缓冲区管理：

    + `LRU`
    + `LIRS`：将缓冲池分为两级，数据首先进入第一级，如果数据在较短时间内被访问两次或以上，则成为热点数据进入第二级，每一级内部还是采用`LRU`算法。这样可以避免全表扫描污染缓冲池。

+ `LSM`树存储引擎

  + 