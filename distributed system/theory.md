## 数据分片与路由

+ **横向扩展**：提高单机硬件资源配合来解决问题。
+ **水平扩展：**增加机器的数量。
+ 对于待处理的海量数据，需要通过数据分片将数据进行切分，并分片到各个机器中；分片之后，需要找到某条记录
  的存储位置，这被称为数据路由。
+ 对于海量数据，通过数据分片实现系统的水平扩展，而通过数据复制来保证数据的高可用。
+ 通过将一份数据复制多次，提高系统的可用性；数据复制还可以增加读的效率，客户端可以从多个备份数据中选择物理距离较近的进行读取，即增加了读操作的并发性又可以提高单次读的效率。
+ 由于每份数据存在多个副本，在并发对数据进行更新时如何保证数据的一致性就成为了关键问题。
+ 常见的分片方式有：**哈希分片、范围分片**等

### 抽象模型

+ 实际上是一个**二级映射**关系：
  + 第一级映射是`key-partition`映射，其将数据记录映射到数据分片空间，这往往是多对一的映射关系，即一个数
    据分片包含多天记录数据；
  + 第二级映射是`partition-machine`映射，其将数据分片映射到物理机器中，这一版也是多对一映射关系，即一台
    物理主机容纳多个数据分片。
  + 在做数据分片时，根据`key-partition`映射关系将大数据水平切割成中多的数据分片，然后再按照`partition-machine`映射关系将数据分片放置到对应的物理机器上。
  + 在数据路由时，比如要查找某条记录的值`get(key)`，首先根据`key-partition`映射找到对应的数据分片，然后再查找`partition-machine`关系表，就可以知道具体哪台物理机器存储该条数据，之后即可从相应的物理机读取`key`对应的`value`内容

### 哈希取模法

+ `Round Robin`就是 哈希取模法。假设有K台物理机，通过以下哈希函数即可实现数据分片： `H(key)=hash(key)modK`对物理机进行编号`0`到`K-1`，根据以上哈希函数，对于以`key`为主键的某个记录，`H(key)`的数值即是物理机在在集群中的放置位置（编号）。
+ 抽象模型中第二级`partion-machine`映射为一对一映射。
+ 问题：**缺乏灵活性**，因为一旦集群中加入了某台机器或减少某台机器都会导致映射关系被打乱，需要重新分片。

### 虚拟桶

+ 为了解决哈希取模法缺乏灵活性的问题，在Round Robin基础上， 加入了一个“虚拟桶层”。所有记录都通过哈希函数映射到对应的虚拟桶中（多对一映射）。虚拟桶和物理机之间再由一层映射。

  ![1564623143898](https://ws1.sinaimg.cn/large/77451733gy1g5jxc3iwqhj20g20d73zj.jpg)

#### redis中分片的实现

+ **todo**

### 一致性哈希（Consistent Hashing）

+ 判断哈希算法好坏
  + **平衡性：**指的是哈希的结果应该尽可能负载均衡地分布到所有的节点中去，这样可以充分利用所有的节点空间。
  + **单调性：**指的是如果已经有一些内容通过哈希分配到了相应分片中，此时又有新的分片加入系统中，这可能导致哈希算法改变，哈希的结果应该保证原本已分配的数据被映射到原有的分片或新的分片中，而不会映射到旧分片集合中的其他分片。
  + **分散性：**在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致**相同内容被存储到不同缓冲中去，降低了系统存储的效率。**分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。
  + **负载：**负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么**对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容**。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。
+ 采用**哈希取模法**中，当有节点新增或者下线时，会**违反单调性**原则。
+ 环形`Hash`空间
  + 按照一定的哈希算法将对应的`key`值哈希到一个具有`2^32`个桶的空间中，即`0~（2^32-1）`。这个集合集合首尾相连，形成一个环。
  + 对象通过哈希函数计算出对应的`key`值，然后散列到环上。
  + 机器加入集群时，也需要通过哈希算法映射到环中（一般情况下，使用机器的IP地址作为哈希算法的输入）。环中的对象，存储于顺时针方向离自己最近的节点中。
+ 机器增加
  + 当往集群中添加一个新的节点，通过哈希算法得到哈希值`NODE-NEW`，并映射到环中。此时涉及到数据迁移。新节点的加入，会将原有的节点`NODE-OLD`的值域空间一分为二，其中顺时针方向离`NODE_NEW`更近的数据需要从`NODE-OLD`迁移至`NODE-NEW`。
  + 一致性算法在**保持了单调性**的同时，尽量减少了数据的迁移，减少了服务器的压力，适合于分布式集群。
+ 机器删除
  + 删除节点`NODE`时，其上数据顺时针迁移到下一个节点`NODE`中。
+ **平衡性**
  + 上述描述的一致性算法，可能会导致负载不均衡。在一致性哈希算法中，为了尽量满足负载均衡，引入了**虚拟节点**的概念。
  + 虚拟节点可以理解为一层抽象，每台机器/节点会包含若干虚拟节点，虚拟节点中存储数据，通过一定的虚拟节点
    分配规则可实现负载均衡。
+ 路由问题
  + 在`p2p`环境中没有中心管理节点，如何根据数据记录的主键以及哈希函数`H`定位到记录的内容？
    + a. 顺序查找
      + 接收到查询请求的节点根据哈希函数，获得待查找主键的哈希值`j`，首先判断是否在自身管理范围内，如果不在，则转交给后续节点继续查找，如此循环，直到找到某个机器节点`Nx`，`x`是大于等于`j`的最小编号的节点，这样最多需要遍历所有机器节点才会给出查找结果。
      + 顺序查找的时间开销太大：`O(n)`，为了解决方法`a`中时间开销过大的问题，采取路由表的做法。
    + b. 全局路由表
      + 全局路由表即每个节点上都会记录集群中所有节点的哈希值域范围。这样在任意节点上，最多只需遍历一次该全局路由表即可得到目标节点位置。
      + 但存在的问题：
        + 当有新增节点或者删除节点时，所有的节点需要更新该路由表
        + 集群中所有节点都要存储全局路由表，存储开销较大
    + c. 二分路由表法
      + 为了加速查找速度、避免太多存储空间，采用二分路由法。每个机器存储`m`条路由信息（`m`为哈希空间的二进制数值比特位长度）。第`i`条（从`1`开始）记录表示，存储距离自己 `2^(i - 1)` 的位置的节点编号。
      + 查询流程：
        + 接收到查询请求的节点根据哈希函数，获得待查找主键的哈希值j，首先判断是否在 `c<j<=s` （`Nc`为当前节点，`Ns`表示后续节点）。
        + 如果不在，查询该路由表，找到小于j的最大编号节点`Nh`。然后去`Nh`上查询。继续第一步进行查找。

### 范围分片

+ 范围分片中首先需要将所有记录的主键进行排序，然后在排序好的主键空间里将记录划分成数据分片，每个数据分片存储有序的主键空间片段内的所有记录。

## 数据复制与一致性

### CAP理论

+ `Consistency`（一致性）：指数据在多个副本之间能够保持一致的特性（严格的一致性）
+ `Availability`（可用性）：指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）
+ `Partition tolerance`（分区容错性）：分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供的服务，除非整个网络环境都发生了故障
+ 分区的概念
  + 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。
  + 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。

#### 论证

+ 如图所示，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，他们之间网络可以连通，N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzb4axhrj205o06oq37.jpg)

+ 在满足一致性的时候，N1和N2中的数据是一样的，V0=V0。
+ 在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。
+ 在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的
  正常运作。
+ 如图所示，这是分布式系统正常运转的流程，用户向N1机器请求数据更新，程序A更新数据库V0为V1。分布式系统将数据进行同步操作M，将V1同步的N2中V0，使得N2中的数据V0也更新为V1，N2中的数据再响应N2的请求。

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzhxascqj20h306rgn7.jpg)

+ 根据CAP原则定义，系统的一致性、可用性和分区容错性细分如下：
  + 一致性：N1和N2的数据库V之间的数据是否完全一样。
  + 可用性：N1和N2的对外部的请求能否做出正常的响应。
  + 分区容错性：N1和N2之间的网络是否互通。
+ 这是正常运作的场景，也是理想的场景。作为一个分布式系统，它和单机系统的最大区别，就在于网络。现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常。相当于要满足分区容错性，能不能同时满足一致性和可用性呢？还是说要对他们进行取舍？

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzjcx4z4j20h706v0ug.jpg)

+ 假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1。由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0。这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？
+ 这里有两种选择：
  + 第一：牺牲数据一致性，保证可用性。响应旧的数据V0给用户。
  + 第二：牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。
  + 这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。

#### 权衡

+ CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一（即**保证CP或AP**）。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。但是，对于**大多数互联网应用**来说，因为规模比较大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有**舍弃一致性来保证服务的 AP**。但是对于一些**金融相关行业**，它有很多场景需要确保一致性，这种情况通常会**权衡 CA 和 CP** 模型，**CA 模型网络故障时完全不可用**，**CP模型具备部分可用性。**
+ 在一个分布式系统中，对于这三个特性，我们只能三选二，无法同时满足这三个特性，三选二的组合以及这样系统
  的特点总结如下：
  + **CA** (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提
    交”（2PC）。CA 系统不能容忍网络错误或节点错误，**一旦出现这样的问题**，整个系统就会拒绝写请求，因为
    它并不知道对面的那个结点是否挂掉了，还是只是网络问题。**唯一安全的做法就是把自己变成只读的**。
  + **CP** (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，
    比如：**Paxos** 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有
    同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。
  + **AP** (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致
    性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。
+ 对于分布式系统分区容忍性是天然具备的要求，否则一旦出现网络分区，系统就拒绝所有写入只允许可读，这对大
  部分的场景是不可接收的，因此，在设计分布式系统时，更多的情况下是选举 CP 还是 AP，要么选择强一致性弱可
  用性，要么选择高可用性容忍弱一致性。
+ 传统的CAP具有误导性：
  + 在实际系统中，网络分区出现的概率很小，应该尽可能兼顾CAP，而不是在设计之初为了容忍小概率事件就放弃A或C。
  + 必须在AC之间做出取舍的时候，不应该粗粒度地在整个系统级别进行取舍。应该考虑系统中的不同子系统，
  + 即对不同子系统采取不同的取舍策略，不同的系统运行时对不同数据采取不同的取舍策略。
  + **CAP三者并非绝对的有或没有，应该看作连续变量。**
+ 推荐的CAP策略：
  + 在绝大多数系统未产生网络分区的情况下，应该尽可能保证AC两者兼得，也即大多数情况下应该保证CAP兼得；当网络分区发生之后，系统应该识别这种状况并对其进行正确的处理，具体而言，分为三个步骤：首先能够识别网络分区的发生，然后在网络分区场景下进入明确的分区模式，此时很可能会限制某些系统操作，最后在网络分区解决后能够进行善后工作，即恢复数据的一致性或者弥补分区模式中产生的错误。

### 一致性模型的分类

![](https://ws1.sinaimg.cn/large/77451733gy1g5jzuvdiddj20k4092dih.jpg)

+ 强一致性

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5jzx5xyk4j20di063acf.jpg)

  + 对于连接到数据库的所有进程，看到的关于某数据的数据值是一致的，如果某进程对数据进行修改，所有进程的后续的读操作就会读到这个更新后的值为基准，直到整个数据被其他进程改变为止。
  + 但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。

+ 弱一致性

  + 不能满足强一致性的情形都可以称为弱一致性。
  + 即系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。

+ 最终一致性

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5jzyxyajij20dn06r0vz.jpg)

  + 最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一段时
    间，在这个时间之后可以保证这一点，而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为**不一致窗口**。
  + 不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。
  + 最终一致性在实际应用中又有多种变种
    + **因果一致性**：如果 A 进程在更新之后向 B 进程通知更新的完成，那么 B 的访问操作将会返回更新的值。而没有因果关系的 C 进程将会遵循最终一致性的规则（C 在不一致窗口内还是看到是旧值）。
    + **读你所写一致性**：因果一致性的特定形式。一个进程进行数据更新后，会给自己发送一条通知，该进程后续的操作都会以最新值作为基础，而其他的进程还是只能在不一致窗口之后才能看到最新值。
    + **会话一致性：**读你所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程可以读取到最新值，但如果会话终止，重新连接后，如果此时还在不一致窗口内，还是可能读取到旧值。
    + **单调读一致性：**如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。
    + **单调写一致性：**系统保证对同一个进程的写操作串行化。

### 两阶段提交（2PC）

+ 二阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保**证在分布式事务中**，要么所有参与进程都提交事务，要么都取消事务，即**实现 ACID 的原子性（A**）。**在数据一致性中**，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来**保证数据的强一致性**。

+ 2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： **参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。**

+ 顾名思义，`2PC` 分为两个过程：

  ![](https://ws1.sinaimg.cn/large/77451733gy1g5k0m0vg6kj21kx0nxgpv.jpg)

  + **表决阶段**：此时 `Coordinator` （协调者）向所有的参与者发送一个 `vote request`，参与者在收到这请求后，如果准备好了就会向 `Coordinator` 发送一个 `VOTE_COMMIT` 消息作为回应，告知 `Coordinator` 自己已经做好了准备，否则会返回一个 `VOTE_ABORT` 消息；
  + **提交阶段**：`Coordinator` 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么`Coordinator` 就会发送 `GLOBAL_COMMIT` 消息，否则发送 `GLOBAL_ABORT` 消息；对于参与者而言，如果收到`GLOBAL_COMMIT` 消息，就会提交本地事务，否则就会取消本地事务。

+ 系统中存在三个阻塞状态：

  + 协调者的`WAIT`状态，参与者的`INIT`状态和`READY`状态。因为这三个状态都在等待对方的反馈消息。

+ 如何避免或者减少进程崩溃导致处于阻塞态的对象进入长时间的等待？

  + 超时判断机制，可以解决协调者的`WAIT`状态和参与者的`INIT`的长时间阻塞。
  + 参与者互询机制，可以解决大部分情况下参与者的`READY`状态长时间阻塞的可能。

+ 超时判断机制：

  + 如果协调者处于`WAIT`状态，加入超时判断机制，可以假定参与者发生崩溃或者存在网络通信故障限制，协调者可以多播`Global-abort`消息取消事务。
  + 如果参与者处于`INIT`状态，说明参与者在等待协调者发出的`Vote-request`消息，超时未收到消息，可以在本地中止事务，向协调者发送`Vote-abort`消息。
  + 如果参与者处于`READY`状态，此时不能直接使用超时判断机制，因为超时直接中止事务可能导致事务处于不一致：协调者实际上发送了`Global-commit`，其他参与者已经提交了事务。
  + 处于`READY`状态并发现超时的参与者必须要搞清除协调者发出的是那种消息，引入**参与者互询**规则可以减缓这一问题。陷入困境的参与者`P`可以询问另外的参与者`Q`，根据`Q`的状态来决定自己应该做什么。
    + 如果参与者`Q`处于`COMMIT`状态，说明协调者已经`Global-commit`，此时`P`可以安全地提交。
    + 如果参与者`Q`处于`ABORT`状态，说明协调者已经`Global-abort`，此时`P`可以安全地将自身状态转换`ABORT`。
    + 如果参与者`Q`处于`INIT`状态，如果考虑到协调者超时机制，最后该事务一定会进入`ABORT`状态，此时`P`可以安全地将自身状态转换成`ABORT`。
    + 如果参与者`Q`处于`READY`状态，此时无法从`Q`获得有效信息，询问其他节点。如果所有参与者都是处于`READY`状态，所有参与者必须处于阻塞状态，等待崩溃的协调者重新启动。即出现“**长时间阻塞**”。

#### 2PC一致性问题

+ 2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。
  + Coordinator 挂了，参与者没挂
    + 这种情况其实比较好解决，只要找一个 Coordinator 的替代者。当他成为新的 Coordinator的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。
  + 参与者挂了（无法恢复），Coordinator 没挂
    + 如果挂了之后没有恢复，那么是不会导致数据一致性问题。
  + 参与者挂了（后来恢复），Coordinator 没挂
    + 恢复后参与者如果发现有未执行完的事务操作，直接取消，然后再询问 Coordinator 目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。
  + 参与者挂了，Coordinator 也挂了
    + Coordinator 和参与者在第一阶段挂了
      + 